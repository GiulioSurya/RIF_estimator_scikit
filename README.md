# Residual Isolation Forest (RIF)

**Residual Isolation Forest (RIF)** is a scikitâ€‘learnâ€‘compatible estimator for **contextual anomaly detection (CAD)**.
It augments classic Isolationâ€¯Forest by first modelling the expected behaviour via contextual regression, then detecting anomalies on the *residuals* of that regression.

This repository provides two building blocks:

| Module                    | Role                                                                                                                     |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `ResidualGenerator`       | Fits a Randomâ€¯Forest *env â†’ ind* to obtain **leakageâ€‘free residuals** using one of three strategies (None, OOB, Kâ€‘fold). |
| `ResidualIsolationForest` | Applies Isolationâ€¯Forest on those residuals to flag outliers.                                                            |

---

## ğŸ” Motivation

Raw behavioural signals often drift just because the environment changes. Treating every deviation as an anomaly triggers many false alarms.
RIF embraces **contextual anomaly detection**:

1. **Context learning** â€“ regress behavioural variables *Y* on environmental variables *X*.
2. **Residual extraction** â€“ compute `R = Y âˆ’ Å¶` as behaviour unexplained by the context.
3. **Anomaly detection** â€“ run Isolationâ€¯Forest on *R*.

This decouples legitimate contextual variability from true anomalies.

---

## ğŸ§  How it works

### Endâ€‘toâ€‘end pipeline

```text
fit(X_train)
â””â”€â–¶ ResidualGenerator.fit_transform(X_train)
     â”œâ”€ Fit Randomâ€¯Forest envâ†’ind (one RF per Y)
     â””â”€ Compute residuals_train      â† strategyâ€‘dependent
â””â”€â–¶ IsolationForest.fit(residuals_train)

predict(X_test)
â””â”€â–¶ ResidualGenerator.transform(X_test)
     â””â”€ Compute residuals_test       â† always outâ€‘ofâ€‘sample
â””â”€â–¶ IsolationForest.predict(residuals_test)
```

### Residual strategies

| Strategy               | How residuals\_train are computed                                                                                   | Pros                                                   | Cons                                                                                                                                      |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **`"none"`** (leakage) | RF predicts the very same samples it was fitted on. Residuals for normal points collapse around 0.                  | Fast; best recall if anomalies are strongly separated. | Heavy **dataâ€‘leakage**: threshold learned on an artefact; may explode false positives or false negatives when signal/noise ratio changes. |
| **`"oob"`**            | Uses the *outâ€‘ofâ€‘bag* predictions of each tree. Every training row is predicted by trees that have **not** seen it. | Leakageâ€‘free, quick, no extra splits.                  | Can contain `NaN` when RF has few trees; residuals noisier â‡’ recall drops unless threshold is tuned.                                      |
| **`"kfold"`**          | Kâ€‘fold crossâ€‘validation: each fold is predicted by a model trained on the others.                                   | Fully leakageâ€‘free, deterministic, no `NaN`.           | Slow (trains K RFs), larger memory footprint.                                                                                             |

> **Where it acts**: the strategy only affects **`ResidualGenerator.fit_transform()`** â€“ i.e. the residuals used to *train* the Isolationâ€¯Forest.
> At prediction time (`transform()`), residuals are always computed *outâ€‘ofâ€‘sample* because the test set is new.

### Choosing a strategy

| If you wantâ€¦                                              | Pick    | Tune                                                                                  |
| --------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------- |
| Maximum speed & highest recall **and** you accept leakage | `none`  | Use validation PRâ€‘curve to set a custom threshold instead of default `contamination`. |
| Balanced tradeâ€‘off, leakageâ€‘free                          | `oob`   | Increase `n_estimators` (â‰¥200) to reduce `NaN`; maybe scale residuals.                |
| Production robustness & reproducibility                   | `kfold` | Choose `k`; expect `kÃ—` training time.                                                |

---

## ğŸ“¦ Installation

```bash
pip install git+https://github.com/GiulioSurya/RIF_estimator_scikit.git
```

---

## âš™ï¸ Quick start

```python
from rif_estimator import ResidualIsolationForest

IND_COLS = ["ind_Y0", "ind_Y1", "ind_Y2"]
ENV_COLS = ["env_X0", "env_X1", "env_X2", "env_X3", "env_X4", "env_X5"]

rif = ResidualIsolationForest(
    ind_cols=IND_COLS,
    env_cols=ENV_COLS,
    contamination=0.20,
    residual_strategy="oob",   # "none" or "kfold" also allowed
    bayes_search=True,          # Bayesian tuning of each RF
    iso_params={"max_features": 1},
)

rif.fit(X_train)
labels = rif.predict(X_test)          # -1 = outlier, 1 = inlier
scores = rif.decision_function(X_test)
```

---

## ğŸ—‚ï¸ Contextual vsÂ Behavioural variables

| Term                                                   | Meaning                                                                                                                                                  | Example                                                             |
| ------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **Environmental / Contextual variables**<br>`env_cols` | Features that describe the *context* in which the system operates. They are **explanatory inputs** for the regression step.                              | *Temperature*, *workâ€‘load*, *incoming traffic*, *ambient humidity*. |
| **Behavioural / Indicator variables**<br>`ind_cols`    | Signals that quantify the systemâ€™s *behaviour* and are the **targets** of the regression. Deviations from their expected value flag potential anomalies. | *Energy consumption*, *CPU usage*, *response time*.                 |

**Example**
Imagine a dataâ€‘centre server:

```text
env_cols = [
    "ambient_temp",     # Â°C, measured by room sensors
    "cpu_load",         # %
    "network_in",       # Mbps incoming
]
ind_cols = [
    "power_draw"        # Watts absorbed by the server
]
```

`ResidualGenerator` learns *power\_draw â‰ˆ f(temp, load, net)*; any large residual suggests abnormal power behaviour given the current context.

---

## ğŸ› ï¸ Feature highlights

* **Scikitâ€‘learn API** (`fit`, `predict`, `decision_function`)
* **Leakageâ€‘free residuals** via OOB or Kâ€‘fold
* **Bayesian hyperâ€‘parameter optimisation** (skopt `BayesSearchCV`)
* **Multiâ€‘output ready** (multiple behavioural `Y`)
* **Hashâ€‘based cache** â€“ avoids recomputing residuals when the same DataFrame is passed to `transform()`.

---

## ğŸ“ˆ Best practices

| What                                                                       | Why                                                                   |
| -------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **Standardise residuals** (e.g. `StandardScaler`) before passing to RIF    | Residual scale varies across datasets; stabilises the IF threshold.   |
| **Calibrate `contamination`** on a labelled validation set or via PRâ€‘curve | Default 0.10/0.20 may be subâ€‘optimal when anomaly prevalence changes. |
| **Use leakageâ€‘free strategies in production**                              | Ensures train/test consistency; results less datasetâ€‘dependent.       |

---

## ğŸ§ª Typical useâ€‘cases

* Industrial condition monitoring
* Smartâ€‘grid energy analytics
* Environmental sensor networks
* Behavioural modelling with covariates

---

## ğŸ“š References

* Song etâ€¯al., "Conditional Anomaly Detection" (2007)
* Calikus etâ€¯al., "ConQuest: Contextual Anomaly Detection" (2020)

---

## ğŸ“œ License

**RIF Endâ€‘User License Agreement (RIFâ€‘EULA)**

You are granted a **nonâ€‘exclusive, nonâ€‘transferable** right to **use** this software for internal research, experimentation, or educational purposes.

You may **NOT**:

* redistribute or sublicense the source code or binaries,
* modify the source code and distribute the modified version,
* incorporate the software into proprietary products for commercial sale,
* claim ownership or remove copyright notices,
* hold the author liable for any direct or indirect damage arising from the use of the software.

For any use beyond the rights explicitly granted above, you must obtain prior written permission from the author.

Copyright Â©Â 2025â€¯GiulioÂ SuryaÂ LoÂ Verde. **All Rights Reserved.**
