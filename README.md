# Residual Isolation Forest (RIF)

**Residual Isolation Forest (RIF)** is a scikit‚Äëlearn‚Äëcompatible estimator for **contextual anomaly detection (CAD)**.
It augments classic Isolation‚ÄØForest by first modelling the expected behaviour via contextual regression, then detecting anomalies on the *residuals* of that regression.

This repository provides two building blocks:

| Module                    | Role                                                                                                                     |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `ResidualGenerator`       | Fits a Random‚ÄØForest *env ‚Üí ind* to obtain **leakage‚Äëfree residuals** using one of three strategies (None, OOB, K‚Äëfold). |
| `ResidualIsolationForest` | Applies Isolation‚ÄØForest on those residuals to flag outliers.                                                            |

---

## üîç Motivation

Raw behavioural signals often drift just because the environment changes. Treating every deviation as an anomaly triggers many false alarms.
RIF embraces **contextual anomaly detection**:

1. **Context learning** ‚Äì regress behavioural variables *Y* on environmental variables *X*.
2. **Residual extraction** ‚Äì compute `R = Y ‚àí ≈∂` as behaviour unexplained by the context.
3. **Anomaly detection** ‚Äì run Isolation‚ÄØForest on *R*.

This decouples legitimate contextual variability from true anomalies.

---

## üß† How it works

### End‚Äëto‚Äëend pipeline

```text
fit(X_train)
‚îî‚îÄ‚ñ∂ ResidualGenerator.fit_transform(X_train)
     ‚îú‚îÄ Fit Random‚ÄØForest env‚Üíind (one RF per Y)
     ‚îî‚îÄ Compute residuals_train      ‚Üê strategy‚Äëdependent
‚îî‚îÄ‚ñ∂ IsolationForest.fit(residuals_train)

predict(X_test)
‚îî‚îÄ‚ñ∂ ResidualGenerator.transform(X_test)
     ‚îî‚îÄ Compute residuals_test       ‚Üê always out‚Äëof‚Äësample
‚îî‚îÄ‚ñ∂ IsolationForest.predict(residuals_test)
```

### Residual strategies

| Strategy               | How residuals\_train are computed                                                                                   | Pros                                                   | Cons                                                                                                                                      |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **`"none"`** (leakage) | RF predicts the very same samples it was fitted on. Residuals for normal points collapse around 0.                  | Fast; best recall if anomalies are strongly separated. | Heavy **data‚Äëleakage**: threshold learned on an artefact; may explode false positives or false negatives when signal/noise ratio changes. |
| **`"oob"`**            | Uses the *out‚Äëof‚Äëbag* predictions of each tree. Every training row is predicted by trees that have **not** seen it. | Leakage‚Äëfree, quick, no extra splits.                  | Can contain `NaN` when RF has few trees; residuals noisier ‚áí recall drops unless threshold is tuned.                                      |
| **`"kfold"`**          | K‚Äëfold cross‚Äëvalidation: each fold is predicted by a model trained on the others.                                   | Fully leakage‚Äëfree, deterministic, no `NaN`.           | Slow (trains K RFs), larger memory footprint.                                                                                             |

> **Where it acts**: the strategy only affects **`ResidualGenerator.fit_transform()`** ‚Äì i.e. the residuals used to *train* the Isolation‚ÄØForest.
> At prediction time (`transform()`), residuals are always computed *out‚Äëof‚Äësample* because the test set is new.

### Choosing a strategy

| If you want‚Ä¶                                              | Pick    | Tune                                                                                  |
| --------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------- |
| Maximum speed & highest recall **and** you accept leakage | `none`  | Use validation PR‚Äëcurve to set a custom threshold instead of default `contamination`. |
| Balanced trade‚Äëoff, leakage‚Äëfree                          | `oob`   | Increase `n_estimators` (‚â•200) to reduce `NaN`; maybe scale residuals.                |
| Production robustness & reproducibility                   | `kfold` | Choose `k`; expect `k√ó` training time.                                                |

---

## üì¶ Installation

```bash
pip install git+https://github.com/GiulioSurya/RIF_estimator_scikit.git
```

---

## ‚öôÔ∏è Quick start

```python
from rif_estimator import ResidualIsolationForest

IND_COLS = ["ind_Y0", "ind_Y1", "ind_Y2"]
ENV_COLS = ["env_X0", "env_X1", "env_X2", "env_X3", "env_X4", "env_X5"]

rif = ResidualIsolationForest(
    ind_cols=IND_COLS,
    env_cols=ENV_COLS,
    contamination=0.20,
    residual_strategy="oob",   # "none" or "kfold" also allowed
    bayes_search=True,          # Bayesian tuning of each RF
    iso_params={"max_features": 1},
)

rif.fit(X_train)
labels = rif.predict(X_test)          # -1 = outlier, 1 = inlier
scores = rif.decision_function(X_test)
```

---

## üõ†Ô∏è Feature highlights

* **Scikit‚Äëlearn API** (`fit`, `predict`, `decision_function`)
* **Leakage‚Äëfree residuals** via OOB or K‚Äëfold
* **Bayesian hyper‚Äëparameter optimisation** (skopt `BayesSearchCV`)
* **Multi‚Äëoutput ready** (multiple behavioural `Y`)
* **Hash‚Äëbased cache** ‚Äì avoids recomputing residuals when the same DataFrame is passed to `transform()`.

---
